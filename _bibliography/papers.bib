---
---

@string{aps = {American Physical Society,}}



@INPROCEEDINGS{yu2025dooropen,
  author={Shangqun Yu and Daniel Wu and Sangjun Park and Matthew En Ziyi Zhou and Donghyun Kim},
  booktitle={preprint}, 
  title={Diffusion Policy for Coordinated Control of a Nonholonomic Mobile Base and Dual Arms in Door Opening and Passing}, 
  year={2025},
  volume={},
  number={},
  preview={door_open.gif},
  selected={true}, 
  }


@INPROCEEDINGS{yu2025guidedog,
  author={Shangqun Yu and Hochul Hwang and Trung Dang and Sunghoon Lee and Joydeep Biswas and Nicholas Giudice and Donghyun Kim},
  booktitle={preprint}, 
  title={Human-Centered Development of Guide Dog Robots: Quiet and Stable Locomotion Control}, 
  year={2025},
  volume={},
  number={},
  pages={755-761},
  html={https://drive.google.com/file/d/1FofGMltQUzhwrvF6ont4coRen2lh61t0/view?usp=sharing},
  video={https://drive.google.com/file/d/1g5lhBPawaapRFHhGmQYoo2XbLXBGgOeD/view?usp=sharing},
  preview={guide_dog.gif},
  selected={true}, 
  }


@INPROCEEDINGS{yu2024learninggenericdynamiclocomotion,
  author={Shangqun Yu and Nisal Perera and Daniel Marew and Donghyun Kim},
  booktitle={2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)}, 
  title={Learning Generic and Dynamic Locomotion of Humanoids Across Discrete Terrains}, 
  year={2024},
  volume={},
  number={},
  pages={755-761},
  html={https://arxiv.org/abs/2405.17227},
  video={https://www.youtube.com/watch?v=h0k11Ess_kc},
  preview={tello.gif},
  selected={true}, 
  }

  @INPROCEEDINGS{marew2024biomechanicsinspiredapproachsoccerkicking,
      title={A Biomechanics-Inspired Approach to Soccer Kicking for Humanoid Robots}, 
      booktitle={2024 IEEE-RAS 23rd International Conference on Humanoid Robots (Humanoids)}, 
      author={Daniel Marew and Nisal Perera and Shangqun Yu and Sarah Roelker and Donghyun Kim},
      year={2024},
      eprint={2407.14612},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      html={https://arxiv.org/abs/2407.14612}, 
      video={https://www.youtube.com/watch?v=Tx7TbmW85Yk},
      preview={kick.gif} ,
      selected={true}, 
}

@INPROCEEDINGS{perera2024staccatoesinglelegrobotmimics,
  author={Nisal Perera and Shangqun Yu and Daniel Marew and Mack Tang and Ken Suzuki and Aidan McCormack and Shifan Zhu and Yong-Jae Kim and Donghyun Kim},
  booktitle={2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={StaccaToe: A Single-Leg Robot that Mimics the Human Leg and Toe}, 
  year={2024},
  volume={},
  number={},
  pages={755-761},
  html={https://arxiv.org/abs/2404.05039}, 
  video={https://www.youtube.com/watch?v=jZwrF528Fg0},
  preview={staccatoe.gif},
  selected={true},
  }

@INPROCEEDINGS{10597522,
  author={Guan, Neil and Yu, Shangqun and Zhu, Shifan and Kim, Donghyun},
  booktitle={2024 21st International Conference on Ubiquitous Robots (UR)}, 
  title={Impedance Matching: Enabling an RL-Based Running Jump in a Quadruped Robot}, 
  year={2024},
  volume={},
  number={},
  pages={755-761},
  keywords={Legged locomotion;Training;Impedance matching;Frequency-domain analysis;Reinforcement learning;Dynamic range;Hardware},
  doi={10.1109/UR61395.2024.10597522},
  preview={ur.gif},
  selected={true},
  }

  @INPROCEEDINGS{10375234,
  author={Marew, Daniel and Lvovsky, Misha and Yu, Shangqun and Sessions, Shotaro and Kim, Donghyun},
  booktitle={2023 IEEE-RAS 22nd International Conference on Humanoid Robots (Humanoids)}, 
  title={Integration of Riemannian Motion Policy with Whole-Body Control for Collision-Free Legged Locomotion}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  keywords={Legged locomotion;Tracking;Heuristic algorithms;Dynamics;Humanoid robots;Robustness;Hardware},
  doi={10.1109/Humanoids57100.2023.10375234},
  preview={pat.gif},
  selected={true},
  }

  @article{zhu2023dynamic,
  bibtex_show={true},
  preview={iros-w.gif},
  abstract={As robots increase in agility and encounter fast- moving objects, dynamic object detection and avoidance become notably challenging. Traditional RGB cameras, burdened by motion blur and high latency, often act as the bottleneck. Event cameras have recently emerged as a promising solution for the challenges related to rapid movement. In this paper, we introduce a dynamic object avoidance framework that integrates both event and RGBD cameras. Specifically, this framework first estimates and compensates for the event’s motion to detect dynamic objects. Subsequently, depth data is combined to derive a 3D trajectory. When initiating from a static state, the robot adjusts its height based on the predicted collision point to avoid the dynamic obstacle. Through real-world experiments with the Mini-Cheetah, our approach successfully circumvents dynamic objects at speeds up to 5 m/s, achieving an 83% success rate.
Supplemental video: },
  title={Dynamic Object Avoidance using Event-Data for a Quadruped Robot},
  author={Zhu, Shifan and Perera, Nisal and Yu, Shangqun and Hwang, Hochul and Kim, Donghyun},
  journal={IROS IPPC Workshop},
  year={2023},
  video={https://www.youtube.com/watch?v=wEPvynkVlLA},
  html={https://ippc-iros23.github.io/papers/zhu.pdf},
  selected={true},
}



@InProceedings{pmlr-v202-fu23f,
  title = 	 {Meta-learning Parameterized Skills},
  author =       {Fu, Haotian and Yu, Shangqun and Tiwari, Saket and Littman, Michael and Konidaris, George},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {10461--10481},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/fu23f/fu23f.pdf},
  url = 	 {https://proceedings.mlr.press/v202/fu23f.html},
  abstract = 	 {We propose a novel parameterized skill-learning algorithm that aims to learn transferable parameterized skills and synthesize them into a new action space that supports efficient learning in long-horizon tasks. We propose to leverage off-policy Meta-RL combined with a trajectory-centric smoothness term to learn a set of parameterized skills. Our agent can use these learned skills to construct a three-level hierarchical framework that models a Temporally-extended Parameterized Action Markov Decision Process. We empirically demonstrate that the proposed algorithms enable an agent to solve a set of highly difficult long-horizon (obstacle-course and robot manipulation) tasks.},
  preview={meta.gif},
  selected={true}, 
}

@inproceedings{NEURIPS2022_d0cf8992,
 abbr={NeurIPS},
 author = {Fu, Haotian and Yu, Shangqun and Littman, Michael and Konidaris, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {32369--32382},
 publisher = {Curran Associates, Inc.},
 title = {Model-based Lifelong Reinforcement Learning with Bayesian Exploration},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/d0cf89927acd9136d27ebf08f9e8a888-Paper-Conference.pdf},
 volume = {35},
 year = {2022},
 preview = {mdb.png},
 selected={true}, 
}

@article{Lobel_Rammohan_He_Yu_Konidaris_2023, 
 abbr={AAAI},
 title={Q-functionals for Value-Based Continuous Control}, 
 volume={37}, 
 url={https://ojs.aaai.org/index.php/AAAI/article/view/26073}, 
 DOI={10.1609/aaai.v37i7.26073}, 
 abstractNote={We present Q-functionals, an alternative architecture for continuous control deep reinforcement learning. Instead of returning a single value for a state-action pair, our network transforms a state into a function that can be rapidly evaluated in parallel for many actions, allowing us to efficiently choose high-value actions through sampling. This contrasts with the typical architecture of off-policy continuous control, where a policy network is trained for the sole purpose of selecting actions from the Q-function. We represent our action-dependent Q-function as a weighted sum of basis functions (Fourier, Polynomial, etc) over the action space, where the weights are state-dependent and output by the Q-functional network. Fast sampling makes practical a variety of techniques that require Monte-Carlo integration over Q-functions, and enables action-selection strategies besides simple value-maximization. We characterize our framework, describe various implementations of Q-functionals, and demonstrate strong performance on a suite of continuous control tasks.}, number={7}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
 author={Lobel, Samuel and Rammohan, Sreehari and He, Bowen and Yu, Shangqun and Konidaris, George}, 
 year={2023}, 
 month={Jun.}, 
 pages={8932-8939},
 preview = {qfunction.png},
 selected={true}, 
 }

 @inproceedings{HierarchicalRL,
 abbr={RLDM},
 author = {Yu, Shangqun and Rammohan, Sreehari and Zheng, Kaiyu and Konidaris, George},
 booktitle = {Multidisciplinary Conference on Reinforcement Learning and Decision Making},
 title = {Hierarchical Reinforcement Learning of Locomotion Policies in Response to Approaching Objects: A Preliminary Study},
 pages = {22--33},
 url = {https://arxiv.org/abs/2203.10616},
 volume = {31},
 year = {2022},
 preview = {hrl.png}
}

@article{BAKER2023274,
abbr={Neural Networks},
title = {A domain-agnostic approach for characterization of lifelong learning systems},
journal = {Neural Networks},
volume = {160},
pages = {274-296},
year = {2023},
issn = {0893-6080},
doi = {https://doi.org/10.1016/j.neunet.2023.01.007},
url = {https://www.sciencedirect.com/science/article/pii/S0893608023000072},
author = {Megan M. Baker and Alexander New and Mario Aguilar-Simon and Ziad Al-Halah and Sébastien M.R. Arnold and Ese Ben-Iwhiwhu and Andrew P. Brna and Ethan Brooks and Ryan C. Brown and Zachary Daniels and Anurag Daram and Fabien Delattre and Ryan Dellana and Eric Eaton and Haotian Fu and Kristen Grauman and Jesse Hostetler and Shariq Iqbal and Cassandra Kent and Nicholas Ketz and Soheil Kolouri and George Konidaris and Dhireesha Kudithipudi and Erik Learned-Miller and Seungwon Lee and Michael L. Littman and Sandeep Madireddy and Jorge A. Mendez and Eric Q. Nguyen and Christine Piatko and Praveen K. Pilly and Aswin Raghavan and Abrar Rahman and Santhosh Kumar Ramakrishnan and Neale Ratzlaff and Andrea Soltoggio and Peter Stone and Indranil Sur and Zhipeng Tang and Saket Tiwari and Kyle Vedder and Felix Wang and Zifan Xu and Angel Yanguas-Gil and Harel Yedidsion and Shangqun Yu and Gautam K. Vallabha},
keywords = {Lifelong learning, Reinforcement learning, Continual learning, System evaluation, Catastrophic forgetting},
preview = {lifelong.png},
abstract = {Despite the advancement of machine learning techniques in recent years, state-of-the-art systems lack robustness to “real world” events, where the input distributions and tasks encountered by the deployed systems will not be limited to the original training context, and systems will instead need to adapt to novel distributions and tasks while deployed. This critical gap may be addressed through the development of “Lifelong Learning” systems that are capable of (1) Continuous Learning, (2) Transfer and Adaptation, and (3) Scalability. Unfortunately, efforts to improve these capabilities are typically treated as distinct areas of research that are assessed independently, without regard to the impact of each separate capability on other aspects of the system. We instead propose a holistic approach, using a suite of metrics and an evaluation framework to assess Lifelong Learning in a principled way that is agnostic to specific domains or system techniques. Through five case studies, we show that this suite of metrics can inform the development of varied and complex Lifelong Learning systems. We highlight how the proposed suite of metrics quantifies performance trade-offs present during Lifelong Learning system development — both the widely discussed Stability-Plasticity dilemma and the newly proposed relationship between Sample Efficient and Robust Learning. Further, we make recommendations for the formulation and use of metrics to guide the continuing development of Lifelong Learning systems and assess their progress in the future.}
}


